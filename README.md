# E-Commerce Data Engineering Project (Portfolio Version)

This is a simulated version of an enterprise-level e-commerce data pipeline I worked on professionally.  
It demonstrates how to build a scalable, secure, and analytics-ready data platform using Azure.

---

## ğŸ›  Tools Used

- Azure Data Factory (ADF)
- Azure Data Lake Storage Gen2
- Azure Synapse Analytics (Notebooks, SQL Pools)
- PySpark
- Delta Lake
- Power BI (for final reporting â€“ not included here)

---

## ğŸ—ï¸ Architecture

We followed the Medallion Architecture:

### Bronze Layer
- Ingest raw CSV/JSON files from sources into Azure Data Lake (ADLS Gen2)
- Tools: ADF Pipelines

### Silver Layer
- Clean, deduplicate, enrich, and standardize data
- Tools: Synapse Notebooks using PySpark

### Gold Layer
- Aggregate data for reporting (daily sales, top products, etc.)
- Tools: Delta Tables + Synapse SQL Pools

---

## ğŸ” Features

- Secure authentication (Service Principal)
- Delta Lake for version control
- Data partitioning and caching for performance
- Logging and error handling

---

## ğŸ‘¨â€ğŸ’» My Role

- Designed data models & Medallion Architecture  
- Built ADF pipelines for data ingestion  
- Wrote PySpark code in Synapse for transformation  
- Tuned pipeline performance using partitioning & caching  
- Applied security best practices (RBAC, encryption)

## ğŸ“‚ Folder Structure

ecommerce-data-engineering/
â”œâ”€â”€ bronze/ # Ingestion layer
â”œâ”€â”€ silver/ # Cleaned and transformed data
â”œâ”€â”€ gold/ # Aggregated data for BI
â”œâ”€â”€ README.md # Project description


> âš ï¸ **Note**: This is a simulated project. No confidential code or real client data is used.

---

## ğŸ“ˆ Coming Soon
Sample ADF JSON, PySpark scripts, and fake datasets to help you run the project yourself.

